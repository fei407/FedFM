[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "fedfm"
version = "1.0.0"
description = ""
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.19.0",
    "flwr-datasets>=0.5.0",
    "torch==2.6.0",
    "transformers>=4.50.0,<5.0",
    "evaluate>=0.4.0,<1.0",
    "datasets>=3.0.0",
    "scikit-learn>=1.6.1, <2.0",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "fw407"

[tool.flwr.app.components]
serverapp = "fedfm.server_app:app"
clientapp = "fedfm.client_app:app"

[tool.flwr.app.config]
grpc-max-message-length = 1073741824
enable-parameter-chaining = true
num-server-rounds = 100
strategy.fraction-fit = 0.2
strategy.fraction-evaluate = 0.0

dataset.name = "vicgalle/alpaca-gpt4"
model.name = "HuggingFaceTB/SmolLM2-135M"

fl.rank-choices = "64,16,4"
fl.device-nums = "1,4,5"

fl.fl-method = "nbias"
fl.peft-name = "lora"
fl.scaling-method = "sqrt"

train.save-every-round = 10
train.learning-rate-max = 1e-3
train.learning-rate-min = 2e-5
train.seq-length = 256
train.training-arguments.output-dir = ""
train.training-arguments.learning-rate = ""
train.training-arguments.per-device-train-batch-size = 4
train.training-arguments.gradient-accumulation-steps = 1
train.training-arguments.logging-steps = 100
train.training-arguments.num-train-epochs = 1
train.training-arguments.max-steps = -1
train.training-arguments.save-steps = 1000
train.training-arguments.save-total-limit = 10
train.training-arguments.gradient-checkpointing = true
train.training-arguments.lr-scheduler-type = "constant"
train.training-arguments.bf16 = false
train.training-arguments.fp16 = false

[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.federations.local-simulation]
options.num-supernodes = 10
options.backend.client-resources.num-cpus = 10
options.backend.client-resources.num-gpus = 0.5
